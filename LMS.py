# -*- coding: utf-8 -*-
"""PTC3569_Ex1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16N3E8eWm5FBeGxcWfNKWHE9zckKCfJoN
"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Inteligencia Computacional

sns.set() # Para deixar os gráficos bonitinhos


def J(y_true, y_predict):
    """
    Função que calcula o Erro Médio Quadrático.

    Paramters
    ---------
    y_true : numpy.array
        Array contendo a saída desejada.
    y_predict : numpy.array
        Array contendo a saída do modelo.

    Returns
    -------
    error : float
        Erro Médio Quadrático.
    """

    # calcula o erro de cada saída
    e = y_true.flatten() - y_predict.flatten()

    # calcula o erro total
    error = (1/2) * np.mean(e**2)
    return error

def LMS(data, output, alpha, method="batelada", iter_max=10, show_J=False, show_w=False):
    """
    Função que aplica o método LMS para um conjunto de pontos.

    Parameters
    ----------
    data : numpy.array
        Array contendo os pontos de entrada. A matriz deve ser n x d, onde 'n'
        é o número de pontos e 'd' a dimensão dos pontos.
    output : numpy.array
        Array contendo a saída desejada. Seu tamanho deve ser igual ao número
        de pontos.
    alpha : float
        Taxa de aprendizagem. Deve ser entre 0 e 1 para melhor performance.
    method : string (default = "batelada")
        Qual método utilizado para o treinamento. "batelada" para utilizar todos
        os dados ou "amostra" para utilizar um ponto por vez.
    iter_max : int (default = 10)
        Números de Iterações que o algoritmo vai executar.
    show_J : boolean (default = False)
        Boolean que indica se deve imprimir ou não o gráfico do Custo por
        iteração.
    show_w : boolean (default = False)
        Boolean que indica se deve imprimir ou não o gráfico do peso por
        iteração.

    Returns
    -------
    w : np.array
        Array contendo os pesos e bias resultantes do treino. A posição [0] é
        o valor do bias.
    """

    # adicionando entrada 1 no início dos pontos para w[0] ser o bias
    X = np.insert(data, 0, 1, axis=1)

    # número de linhas e dimensão do vetor de bias + pesos
    n, d = X.shape

    # inicializando o vetor bias + pesos
    w = np.random.random((1, d))

    if show_J:
        J_array = np.zeros(iter_max)

    if show_w:
        w_array = np.zeros((iter_max, d))

    # treinamento
    if method is "batelada":
        for repeat in range(iter_max):

            # calculando o potencial de saída de todos os pontos
            u = X.dot(w.T)

            if show_J:
                # salvando o J para plotar
                J_array[repeat] = J(output, u)

            if show_w:
                # salvando o w para plotar
                w_array[repeat] = w

            # calculando o erro de cada ponto
            e = u - output.reshape(-1,1)

            # calculando o gradiente
            grad = np.mean(e*X, axis=0)

            # atualizando os pesos
            w = w - alpha*grad

    elif method is "amostra":

        for repeat in range(iter_max):

            if show_J:
                # para calcular J
                u = X.dot(w.T)

                # salvando o J para plotar
                J_array[repeat] = J(output, u)

            if show_w:
                # salvando o w para plotar
                w_array[repeat] = w

            # atualiza os pesos para cada ponto
            for i in range(n):

                # calculando o potencial de saída do ponto
                u = X[i].reshape(1,-1).dot(w.T)

                # calculando o erro
                e = u - output[i]

                # calculando o gradiente
                grad = e*X[i]

                # atualizando os pesos
                w = w - alpha*grad
    else:
        print(f"{method} desconhecido. Utilize 'batelada' ou 'amostra'.")


    if show_J:

        plt.plot(np.arange(iter_max), J_array, marker='.', c='black')
        plt.title("Evolução do J")
        plt.ylabel("J")
        plt.xlabel("Iteration")
        plt.show()

    if show_w:
        for i in range(d):
            if i == 0:
                ylabel = "Bias"
            else:
                ylabel = f"Peso {i}"

            plt.plot(np.arange(iter_max), w_array[:,i], marker='.', c='black')
            plt.title(f"Evolução do {ylabel}")
            plt.ylabel(ylabel)
            plt.xlabel("Iteration")
            plt.show()

    return w

### Exercício 2


# a)
print("Parte (a)")
data = np.array([[-0.4], [-0.2], [-0.1], [0.3], [0.6], [0.5], [0.7]])
output = np.array([-1, 1.5, 2, 3.2, 3.5, 5, 2])

w = LMS(data, output, 0.2, method='batelada', iter_max=50, show_J=True, show_w=True)

output_LMS = np.insert(data, 0, 1, axis=1).dot(w.T).sum(axis=1)

print(f"Bias encontrado : {round(w[0,0], 3)}\nPeso encontrado : {round(w[0,1], 3)}")

# Visualização do resultado
plt.scatter(data.flatten(), output, s=10, label='Target')
plt.plot(data.flatten(), output_LMS, marker='.', c='black', label='Predictor')
plt.xlabel("X")
plt.ylabel("d")
plt.legend()
plt.show()

# b)
print("Parte (b)")

n = output.size

X = data.flatten()
Y = output.flatten()

num = n * np.sum(X.flatten() * Y.flatten()) - np.sum(X) * np.sum(Y)
den = np.sqrt( n * np.sum( X**2 ) - np.sum( X )**2 ) * np.sqrt(  n * np.sum( Y**2 ) - np.sum( Y )**2  )

r = num/den

print(f"O coeficiente de correlação é {round(r, 3)}")

# c)

print("Parte (c)")

output_0 = w[0,0] + 0*w[0,1]
output_1 = w[0,0] + 1*w[0,1]

print(f"Para x = 0, a predição (d) é de {round(output_0, 3)}")
print(f"Para x = 1, a predição (d) é de {round(output_1, 3)}")

"""
Como os dados de treino estão no intervalo [-0.4, 0.7], o resultado para x = 0
é de maior confiança, pois x = 1 não está no nosso intervalo.
"""

### Exercício 3

data = np.array([[-0.5, 3],
                 [-0.2, 3],
                 [-0.1, 2.5],
                 [0.3, 2.5],
                 [0.4, -1],
                 [0.6, -1.5],
                 [0.7, -4]])

output = np.array([-3,-1,0,1.2,1.8,3,4])

w = LMS(data, output, 0.2, method='batelada', iter_max=100, show_J=True, show_w=True)

output_LMS = np.insert(data, 0, 1, axis=1).dot(w.T).sum(axis=1)

print(f"Bias encontrado : {round(w[0,0], 3)}\nPeso 1 encontrado : {round(w[0,1], 3)}\nPeso 2 encontrado : {round(w[0,2], 3)}")
